"""
æ¨ç†è¿‡ç¨‹å¯è§†åŒ–ç»„ä»¶

å®Œæ•´å±•ç¤ºæ¨ç†é“¾è·¯çš„æ¯ä¸ªç¯èŠ‚ï¼ŒåŒ…æ‹¬æœ¬ä½“è¿½æº¯ã€‚
"""
import streamlit as st
import pandas as pd
from typing import Dict

from models import DiagnosisResult
from components.ontology_tracer import (
    render_ontology_reference,
    render_synonym_mapping,
    render_mismatch_explanation,
    render_match_type_badge,
    render_confidence_badge,
    render_confidence_level_badge,
)
from config import Q0_QUESTIONS


def render_diagnosis_result(diagnosis_result: DiagnosisResult) -> None:
    """
    æ¸²æŸ“å®Œæ•´çš„æ¨ç†ç»“æœ

    Args:
        diagnosis_result: æ¨ç†ç»“æœå¯¹è±¡
    """
    st.header("ğŸ” æ¨ç†è¿‡ç¨‹å¯è§†åŒ–")

    # 1. Q0åºåˆ—
    render_q0_sequence(diagnosis_result)

    st.divider()

    # 2. Q1-Q6ç‰¹å¾æå–
    render_feature_extraction(diagnosis_result)

    st.divider()

    # 3. å€™é€‰ç–¾ç—…ç­›é€‰
    render_candidate_diseases(diagnosis_result)

    st.divider()

    # 4. åŠ æƒè¯„åˆ†
    render_scoring_results(diagnosis_result)

    st.divider()

    # 5. æœ€ç»ˆè¯Šæ–­
    render_final_diagnosis(diagnosis_result)


def render_q0_sequence(diagnosis_result: DiagnosisResult) -> None:
    """æ¸²æŸ“Q0åºåˆ—ç»“æœ"""
    st.subheader("1ï¸âƒ£ Q0åºåˆ—ï¼ˆ6æ­¥åˆ†ç±»é—®é¢˜ï¼‰")

    for q_key, q_result in diagnosis_result.q0_sequence.items():
        q_config = Q0_QUESTIONS.get(q_key, {})
        label = q_config.get("label", q_key)

        with st.expander(f"**{label}** â†’ {q_result.choice} {render_confidence_badge(q_result.confidence)}", expanded=False):
            col1, col2 = st.columns([2, 1])

            with col1:
                st.markdown(f"**é€‰æ‹©**: `{q_result.choice}`")
                st.markdown(f"**æ¨ç†ç†ç”±**: {q_result.reasoning}")

            with col2:
                st.metric("ç½®ä¿¡åº¦", f"{q_result.confidence:.2f}")

            # æœ¬ä½“å¼•ç”¨
            render_ontology_reference(q_result.ontology_reference)


def render_feature_extraction(diagnosis_result: DiagnosisResult) -> None:
    """æ¸²æŸ“Q1-Q6ç‰¹å¾æå–ç»“æœ"""
    st.subheader("2ï¸âƒ£ Q1-Q6 åŠ¨æ€ç‰¹å¾æå–ï¼ˆ7ä¸ªç‰¹å¾ï¼‰")

    # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®
    table_data = []
    for feature_key, feature_result in diagnosis_result.feature_extraction.items():
        table_data.append({
            "ç‰¹å¾": feature_key,
            "æå–å€¼": feature_result.choice,
            "ç½®ä¿¡åº¦": f"{feature_result.confidence:.2f}",
            "æ¨ç†ç†ç”±": feature_result.reasoning
        })

    df = pd.DataFrame(table_data)
    st.dataframe(df, use_container_width=True, hide_index=True)

    # è¯¦ç»†ä¿¡æ¯ï¼ˆå¯å±•å¼€ï¼‰
    with st.expander("ğŸ”¬ æŸ¥çœ‹ç‰¹å¾æå–è¯¦æƒ…ï¼ˆå«æœ¬ä½“å¼•ç”¨ï¼‰"):
        for feature_key, feature_result in diagnosis_result.feature_extraction.items():
            st.markdown(f"### {feature_key}")
            st.markdown(f"**æå–å€¼**: `{feature_result.choice}` {render_confidence_badge(feature_result.confidence)}")
            st.markdown(f"**æ¨ç†ç†ç”±**: {feature_result.reasoning}")

            # æœ¬ä½“å¼•ç”¨
            render_ontology_reference(feature_result.ontology_reference, f"{feature_key} ç‰¹å¾å®šä¹‰")

            st.divider()


def render_candidate_diseases(diagnosis_result: DiagnosisResult) -> None:
    """æ¸²æŸ“å€™é€‰ç–¾ç—…ç­›é€‰ç»“æœ"""
    st.subheader("3ï¸âƒ£ å€™é€‰ç–¾ç—…ç­›é€‰")

    st.markdown(f"**ç­›é€‰ä¾æ®**: åŸºäº Q0.2 è¯†åˆ«çš„èŠ±å± = `{diagnosis_result.q0_sequence['q0_2_flower_genus'].choice}`")
    st.markdown(f"**å€™é€‰ç–¾ç—…æ•°é‡**: {len(diagnosis_result.candidate_diseases)}")

    # æ˜¾ç¤ºå€™é€‰ç–¾ç—…åˆ—è¡¨
    for i, candidate in enumerate(diagnosis_result.candidate_diseases, 1):
        st.markdown(
            f"{i}. **{candidate.disease_name}** ({candidate.disease_name_en}) - "
            f"`{candidate.ontology_file}` (v{candidate.version})"
        )


def render_scoring_results(diagnosis_result: DiagnosisResult) -> None:
    """æ¸²æŸ“åŠ æƒè¯„åˆ†ç»“æœ"""
    st.subheader("4ï¸âƒ£ æ¨¡ç³ŠåŒ¹é… + åŠ æƒè¯„åˆ†")

    for i, scoring_result in enumerate(diagnosis_result.scoring_results, 1):
        # å¡ç‰‡æ ‡é¢˜
        title = f"ç–¾ç—… {i}: {scoring_result.disease_name} ({scoring_result.disease_id})"

        with st.expander(f"**{title}** - æ€»åˆ†: {scoring_result.total_score:.2f}", expanded=(i == 1)):
            # åŸºæœ¬ä¿¡æ¯
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»åˆ†", f"{scoring_result.total_score:.2f}")
            with col2:
                st.metric("ç½®ä¿¡åº¦çº§åˆ«", scoring_result.confidence_level)
            with col3:
                st.metric("å®Œæ•´æ€§ä¿®æ­£ç³»æ•°", f"{scoring_result.completeness_modifier:.2f}")

            st.markdown(f"**ç–¾ç—…å®šä¹‰æ¥æº**: `{scoring_result.ontology_file}` (v{scoring_result.version})")

            # åˆ†æ•°ç»†åˆ†
            st.markdown("#### ğŸ“Š åˆ†æ•°ç»†åˆ†")
            score_data = [
                {"é‡è¦æ€§çº§åˆ«": "ä¸»è¦ç‰¹å¾ (Major)", "åˆ†æ•°": f"{scoring_result.major_score:.2f}", "æƒé‡": "60%"},
                {"é‡è¦æ€§çº§åˆ«": "æ¬¡è¦ç‰¹å¾ (Minor)", "åˆ†æ•°": f"{scoring_result.minor_score:.2f}", "æƒé‡": "30%"},
                {"é‡è¦æ€§çº§åˆ«": "å¯é€‰ç‰¹å¾ (Optional)", "åˆ†æ•°": f"{scoring_result.optional_score:.2f}", "æƒé‡": "10%"},
            ]
            st.table(pd.DataFrame(score_data))

            # åŒ¹é…è¯¦æƒ…
            st.markdown("#### ğŸ” åŒ¹é…è¯¦æƒ…ï¼ˆå«æœ¬ä½“å¼•ç”¨ï¼‰")

            for match_detail in scoring_result.match_details:
                st.markdown(f"**{match_detail.feature_key}** ({match_detail.importance_level})")

                col1, col2, col3 = st.columns([2, 2, 1])
                with col1:
                    st.markdown(f"VLMè¯†åˆ«: `{match_detail.observed_value}`")
                with col2:
                    st.markdown(f"ç–¾ç—…æœŸæœ›: `{match_detail.expected_value}`")
                with col3:
                    st.markdown(render_match_type_badge(match_detail.match_type))

                st.markdown(f"**è´¡çŒ®åˆ†æ•°**: {match_detail.contribution:.2f}")

                # åŒä¹‰è¯æ˜ å°„æˆ–ä¸åŒ¹é…è¯´æ˜
                if match_detail.match_type == "fuzzy" and match_detail.synonym_mapping:
                    render_synonym_mapping(match_detail.synonym_mapping)
                elif match_detail.match_type == "no_match" and match_detail.mismatch_explanation:
                    render_mismatch_explanation(match_detail.mismatch_explanation)

                # æœ¬ä½“å¼•ç”¨
                if match_detail.ontology_reference:
                    render_ontology_reference(match_detail.ontology_reference, f"{match_detail.feature_key} æœ¬ä½“å®šä¹‰")

                st.divider()


def render_final_diagnosis(diagnosis_result: DiagnosisResult) -> None:
    """æ¸²æŸ“æœ€ç»ˆè¯Šæ–­ç»“æœ"""
    st.subheader("5ï¸âƒ£ æœ€ç»ˆè¯Šæ–­ç»“æœ")

    final = diagnosis_result.final_diagnosis

    # çªå‡ºæ˜¾ç¤º
    st.markdown("### ğŸ¯ è¯Šæ–­ç»“æœ")
    st.markdown(f"## {final.disease_name} ({final.disease_name_en})")

    # ç½®ä¿¡åº¦çº§åˆ«
    render_confidence_level_badge(final.confidence_level, final.confidence_score)

    st.markdown(f"**ç—…åŸä½“**: {final.pathogen}")
    st.markdown(f"**ç–¾ç—…å®šä¹‰**: `{final.ontology_file}` (v{final.version})")

    # æ²»ç–—å»ºè®®
    st.markdown("### ğŸ’Š æ²»ç–—å»ºè®®")
    for i, suggestion in enumerate(final.treatment_suggestions, 1):
        st.markdown(f"{i}. {suggestion}")

    # æ€§èƒ½æŒ‡æ ‡
    with st.expander("â±ï¸ æ€§èƒ½æŒ‡æ ‡"):
        perf = diagnosis_result.performance
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("æ€»è€—æ—¶", f"{perf.total_elapsed_time:.2f}s")
        with col2:
            st.metric("Q0è€—æ—¶", f"{perf.q0_time:.2f}s")
        with col3:
            st.metric("Q1-Q6è€—æ—¶", f"{perf.q1_q6_time:.2f}s")
        with col4:
            st.metric("åŒ¹é…è€—æ—¶", f"{perf.matching_time:.2f}s")

        st.markdown(f"**VLMæä¾›å•†**: {perf.vlm_provider}")
